{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Detector
With this Jupyter notebook you can send requests to a large language model (LLM) via Generative AI Hub on SAP AI Core. Like most other LLM applications, Generative AI Hub operates on a pay-per-use basis.

Generative AI Hub offers all major models on the market. There are open-source models that SAP has deployed such as the Falcon model. And there are models that SAP is a proxy for, such as the GPT models, Google models, models provided by Amazon Bedrock and more. You can easily switch between them, compare results, and select the model that works best for your use case.

SAP maintains strict data privacy contracts with LLM providers to ensure that your data remains secure.

With the code below you can send texts and images to a multimodal LLM. GPT4o, but also Claude and Gemini models can process images as well as text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import json\n",
    "\n",
    "ROOT_PATH_DIR = os.path.dirname(os.getcwd())\n",
    "AICORE_CONFIG_FILENAME = 'service-key.json'\n",
    "with open(os.path.join(ROOT_PATH_DIR, AICORE_CONFIG_FILENAME), 'r') as config_file:\n",
    "    config_data = json.load(config_file)\n",
    "\n",
    "os.environ[\"AICORE_AUTH_URL\"]=config_data[\"url\"]+\"/oauth/token\"\n",
    "os.environ[\"AICORE_CLIENT_ID\"]=config_data[\"clientid\"]\n",
    "os.environ[\"AICORE_CLIENT_SECRET\"]=config_data[\"clientsecret\"]\n",
    "os.environ[\"AICORE_BASE_URL\"]=config_data[\"serviceurls\"][\"AI_API_URL\"]\n",
    "os.environ[\"AICORE_RESOURCE_GROUP\"]=\"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.native.openai import chat\n",
    "\n",
    "with open(\"data/image.png\", \"rb\") as image_file:\n",
    "    image_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What do you see on this image? Only respond with the name of the building.\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{image_data}\"}\n",
    "            }]\n",
    "        }]\n",
    "\n",
    "kwargs = dict(model_name='gpt-4o-mini', messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "\n",
    "print(\"This is the response you are looking for: \" + response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What is this building?\"},\n",
    "            {\"role\": \"assistant\", \"content\": f'''\"That building is '{response.choices[0].message.content}'\"'''},\n",
    "            {\"role\": \"user\", \"content\": \"Ich which city is that building? Only respond with the city and country.\"}]\n",
    "\n",
    "kwargs = dict(model_name='gpt-4o-mini', messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
